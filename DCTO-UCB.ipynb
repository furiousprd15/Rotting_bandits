import numpy as np
import random 
import matplotlib.pyplot as plt

class Bandits():
  def __init__(self,muC,theta,sigma):
    self.muC=np.array(muC)
    self.theta=np.array(theta)
    random.shuffle(self.theta)
    #print(self.theta)
    self.thetaOracle=np.array(theta)
    self.K=len(muC)
    self.sigma=sigma
    self.pullCount=np.zeros(self.K,dtype=float)
    self.rewards=[]
  def pull(self,arm):
    mu=self.muC[arm]+1/(self.pullCount[arm]//100+1)**(self.thetaOracle[arm])
    self.pullCount[arm]+=1
    sample=np.random.normal(mu,self.sigma)
    self.rewards.append(sample)
    #print(sample)
    return  sample
  def reset(self):
    self.pullCount = np.zeros(self.K,dtype=int)
    self.rewards=[]
    return
  def Ddet(self,n,theta1,theta2):#theta1 theta are indexes
    sum=0
    for c1 in range(1,n+1):
      if c1 <=n//2:
        sum+=1/c1**(self.theta[theta1])-1/c1**(self.theta[theta2])
      else:
        sum-=1/c1**(self.theta[theta1])-1/c1**(self.theta[theta2])
    return n*self.sigma**2/sum**2
  def BoundDdetStar(self,delta):
    zeta=1/(8*np.log(2*self.K/delta))
    m=[]
    for theta1 in range(self.K-1):
      for theta2 in range(theta1+1,self.K):
        for c1 in range(1,50):
          Ddet=self.Ddet(2**c1,1,2)
          if(Ddet<=zeta):
            m.append(2**c1)
            break
    return max(m)
  def DCTOUCB(self,Tucb,mDiff):
    r1=np.zeros(self.K,dtype=float)
    r2=np.zeros(self.K,dtype=float)
    u1=np.zeros(self.K,dtype=float)
    u2=np.zeros(self.K,dtype=float)
    for c1 in range(self.K):# round robin 
      for c2 in range(mDiff):
        if(c2<mDiff//2):
          r1[c1]+=self.pull(c1)
          u1[c1]+=1/(c2+1)**self.theta[c1]
        else:
          r2[c1]+=self.pull(c1)
          u2[c1]+=1/(c2+1)**self.theta[c1]  
    # for c1 in range(self.K):
    #   for c2 in range(mDiff):
    #     if(c2<mDiff//2):
    #       u1[c1]+=1/(c2+1)**self.theta[c1]
    #     else:
    #       u2[c1]+=1/(c2+1)**self.theta[c1]


    thetaHat=np.zeros(self.K,dtype=int)
    #finding theta hat
    for c1 in range(self.K):# for each arm
      thetaHat[c1]=0
      Zmin=np.abs(r1[c1]-r2[c1]-u1[0]+u2[0])
      #print(Zmin)
      for c2 in range(1,self.K):
        Z=np.abs(r1[c1]-r2[c1]-u1[c2]+u2[c2])
        #print(Z)
        if(Z<Zmin):
          Zmin=Z
          thetaHat[c1]=c2
    

    CB=np.zeros(self.K,dtype=float)
    for t in range(self.K*mDiff+1,self.K*mDiff+1+Tucb):
      for c2 in range(self.K):
        muCHat=(r1[c2]+r2[c2]-u1[c2]-u2[c2])/self.pullCount[c2]
        confidence=np.sqrt(8*np.log(t)*self.sigma**2/self.pullCount[c2])
        CB[c2]=muCHat+1/(self.pullCount[c2]+1)**(self.theta[thetaHat[c2]])+confidence
      arm=np.argmax(CB)
      r2[arm]+=self.pull(arm)
      u2[arm]+=1/(self.pullCount[arm])**(self.theta[thetaHat[arm]])
    return self.rewards#thetaHat,r1+r2,self.pullCount,

  def optimalPolicy(self,T):
    armCurrentReward=np.zeros(self.K,dtype=float)
    for t in range(T):
      for arm in range(self.K):
        armCurrentReward[arm]=self.muC[arm]+1/(self.pullCount[arm]//100+1)**(self.thetaOracle[arm])
      bestArm=np.argmax(armCurrentReward)
      self.pull(bestArm)
    return self.rewards


muC=[random.random()/2 for c1 in range(4)]
theta=random.choices([0.1+0.05*c1 for c1 in range(8)],k=len(muC))
sigma=0.002
B1=Bandits(muC,theta,sigma)
arm=2
arr=np.zeros([10,5],dtype=float)

Tucb=50000
mdiff=100
l=Tucb+mdiff*len(muC)
RegretsSum=np.zeros(l,dtype=float)
iters=20
for c2 in range(iters):
  dctoRewards=B1.DCTOUCB(Tucb,mdiff)
  B1.reset()
  optRewards=B1.optimalPolicy(l)
  B1.reset()
  preRegret=0
  Regret=np.zeros(l,dtype=float)
  for c1 in range(l):
    Regret[c1]=preRegret+optRewards[c1]-dctoRewards[c1]
    preRegret=Regret[c1]
  RegretsSum+=Regret
RegretAverage=RegretsSum/iters
plt.plot(RegretAverage)
plt.title('Regret with mdiff=100')


